# read Table

mixture.example <- read.table(file = "http://www2.isye.gatech.edu/~ymei/7406/Handouts/mixtureexample.csv", header= TRUE, sep = ",");

# Plot Fig 2.1:

attach(mixture.example); 
plot(x1, x2, col=ifelse(y==1, "red", "green"), xlab="", ylab="")
detach(mixture.example); 

# Linear Regression
mod1 <- lm( y ~ . , data= mixture.example);

#or alternatively, 
attach(mixture.example); 
mod1 <- lm( y ~ x1 + x2);

b0 <- coef(mod1)[1]; b1 <- coef(mod1)[2]; b2 <- coef(mod1)[3];
intercept1 <- (0.5 - b0) / b2;
slope1 <- - b1 / b2;
abline(intercept1, slope1); 

# Training Error (training miscalssification rate)

y1hat <- ifelse( fitted(mod1) > 0.5, 1, 0);
length(y1hat); 
sum(y);
sum(y1hat == y);

1 - sum(y1hat == y) / length(y) ##0.27 (training error)

#*****************
#            y1
#           1     0 
#y1hat=1   76    30
#      0   24    70
#

 
# KNN
#

c(min(x1), max(x1));
c(min(x2), max(x2));
px1 <-  seq(-2.6, 4.2, 0.1);
px2 <-  seq(-2, 2.9, 0.05);

xnew1 <- NULL;
for (i in 1:length(px2)) xnew1 <- rbind(xnew1, cbind(px1, px2[i]));


plot(x1,x2, col=ifelse(y ==1, "red", "green"), xlab="", ylab="")

library(class)
mod2 <- knn(cbind(x1, x2), xnew1, y, k=15, prob=TRUE);
prob1 <- attr(mod2, "prob"); 
prob2 <- ifelse(mod2=="1", 1- prob1, prob1); 
prob3 <- matrix(prob2, length(px1), length(px2));

contour(px1, px2, prob3, level=0.5, labels="", xlab="", ylab="", main="15-NN");
points(x1, x2, col=ifelse(y==1, "red", "green"));


# Test Sample
# since there is no a test sample, so we make one
# using the description on page 17
# we simulate a test sample of size 10000, with 500 obs of each class

library(MASS)
set.seed(123)
centers <- c(sample(1:10, 5000, replace = TRUE), sample(11:20, 5000, replace = TRUE))
means  <- read.table(file = "C:/myajun/Temp/mixturemean.csv", sep = ","); 
means1 <- means[centers,];

testdat <- mvrnorm(10000, c(0,0), 0.2*diag(2));
testdat <- testdat + means1;
colnames(testdat) <- c("x1", "x2");
trueclass <- c(rep(0, 5000), rep(1, 5000)); 

pred <- predict.lm(mod1, testdat);
pred1 <- ifelse(pred> 0.5, 1, 0); 

# test error for linear regression

1 - sum(pred1 == trueclass) / length(trueclass);
sum(pred1 != trueclass) / length(trueclass);
mean( (pred1 - trueclass)^2) 

# 0.2831 (test error)


#*****************
#            trueclass
#           1              0 
#pred1=1   3506(70.1%)    1337(26.7%)
#      0   1494(29.9%)    3663(73.3%)
#
 

### Test error of KNN

pred2 <- NULL;
xx <- cbind(x1, x2);

##
ks <- c(1,3,5,7,9,11,15,17,23,25,35,45,55,83,101, 151);
nks <- length(ks);
misclass.train <- numeric(length= nks);
misclass.test  <- numeric(length= nks);
for (i in 1:nks ){
  mod.train <- knn( xx, xx, k = ks[i], cl = y);
  mod.test  <- knn(xx, testdat, k = ks[i], cl= y); 
  misclass.train[i] <- 1 - sum( mod.train == y) / 200; 
  misclass.test[i]  <- 1 - sum( mod.test == trueclass) / 10000; 
}

cbind(ks, misclass.train, misclass.test)